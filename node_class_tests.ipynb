{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Parameter\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree, homophily\n",
    "\n",
    "from torch_geometric.datasets import WebKB, Planetoid\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GCNConv cell explains how to implement a customized MPNN, with MessagePassing class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages.\n",
    "\n",
    "        ''' self.propagate has to take as input at least 'edge_index' and 'x', then we can specify also other arguments,\n",
    "            like ciao. '''\n",
    "\n",
    "        return self.propagate(edge_index, ciao = 'ciao', size = (x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, x_j, edge_index, ciao, size):\n",
    "        # x_j has shape [E, out_channels]\n",
    "        # Step 3: Normalize node features.\n",
    "        ''' x_j contains the node features for row '''\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == 'inf'] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        print(\"1: \", (norm.view(-1, 1)*x_j).shape)\n",
    "\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        print(\"2: \", aggr_out.shape)\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_texas = WebKB(root='/tmp/Texas', name='Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset_texas.x\n",
    "edges = dataset_texas.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_network = GCNConv(dataset_texas.num_features, dataset_texas.num_classes)\n",
    "t = test_network(x, edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized implementation for the SAGEConv GNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGECv(MessagePassing):\n",
    "    def __init__(self, input_dim, hidden_dim, project = False):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.l1 = torch.nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.l2 = torch.nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.project = project\n",
    "        if self.project:\n",
    "            self.l3 = torch.nn.Linear(input_dim, hidden_dim, bias = True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        if self.project:\n",
    "            x = F.relu(self.l3(x))\n",
    "\n",
    "        return self.propagate(edge_index, x = x)\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out is always the output after the aggregation, and x are the nodes to update.\n",
    "        aggr_out = self.l1(aggr_out)\n",
    "\n",
    "        out = self.l2(x) + self.l1(aggr_out)\n",
    "\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([183, 5])\n",
      "torch.Size([183, 5])\n",
      "torch.Size([183, 5])\n"
     ]
    }
   ],
   "source": [
    "s = SAGECv(dataset_texas.num_features, dataset_texas.num_classes, project = True)\n",
    "\n",
    "t = s(x, edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a linear layer symmetric, info at [this link](https://pytorch.org/tutorials/intermediate/parametrizations.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a square matrix:\n",
    "matrix = torch.tensor([[1, 2, 44], [1, 2, 3111], [0, 1, 4]])\n",
    "# We can get its upper triangular part as \n",
    "print(matrix.triu(0))\n",
    "# and also its counter-version with zero diagonal terms\n",
    "print(matrix.triu(1))\n",
    "# Symmetric matrix is:\n",
    "print(matrix.triu(0) + matrix.triu(1).transpose(-1, -2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to parametrize the linear layers, but also to separate this process by the layer definition we should proceed by using the torch.nn.parametrize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  Parameter containing:\n",
      "tensor([[-0.3834,  0.3018,  0.1072, -0.1389, -0.0130],\n",
      "        [-0.2427, -0.0612, -0.0034, -0.0523, -0.3426],\n",
      "        [ 0.3694,  0.2531,  0.3518,  0.0486,  0.0359],\n",
      "        [-0.2783, -0.1788, -0.0249, -0.4132, -0.1983],\n",
      "        [ 0.4021, -0.1767,  0.1618,  0.3949,  0.1506]], requires_grad=True)\n",
      "AFTER:  tensor([[-0.3834,  0.3018,  0.1072, -0.1389, -0.0130],\n",
      "        [ 0.3018, -0.0612, -0.0034, -0.0523, -0.3426],\n",
      "        [ 0.1072, -0.0034,  0.3518,  0.0486,  0.0359],\n",
      "        [-0.1389, -0.0523,  0.0486, -0.4132, -0.1983],\n",
      "        [-0.0130, -0.3426,  0.0359, -0.1983,  0.1506]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "class Symmetric(torch.nn.Module):\n",
    "    def forward(self, w):\n",
    "        # This class implements the method to define the symmetry in the squared matrices.\n",
    "        return w.triu(0) + w.triu(1).transpose(-1, -2)\n",
    "\n",
    "hidden_dimension = 5\n",
    "\n",
    "# Let's notice that we need to define squared layers\n",
    "layer = torch.nn.Linear(hidden_dimension, hidden_dimension)\n",
    "print(\"BEFORE: \", layer.weight)\n",
    "# LAYER DEFINITION & SYMMETRY are now separated processes.\n",
    "parametrize.register_parametrization(layer, 'weight', Symmetric())\n",
    "print(\"AFTER: \", layer.weight)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see now the symmetry in the GRAFF paper\n",
    "Here the parametrization is done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  Parameter containing:\n",
      "tensor([[-0.0695, -0.0465, -0.3562,  0.2027, -0.1504,  0.1841,  0.1395],\n",
      "        [-0.0970,  0.2512,  0.2056, -0.3359, -0.1418,  0.3448,  0.2772],\n",
      "        [-0.1398, -0.0160,  0.0813, -0.3447, -0.2584,  0.1737, -0.0010],\n",
      "        [ 0.2093,  0.3520, -0.1950, -0.0436, -0.2272,  0.0019,  0.0587],\n",
      "        [ 0.2048, -0.2496, -0.0404, -0.1384,  0.2391,  0.0524, -0.3009]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 0.0000, -0.0465, -0.3562,  0.2027, -0.1504],\n",
      "        [ 0.0000,  0.0000,  0.2056, -0.3359, -0.1418],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.3447, -0.2584],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2272],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<TriuBackward0>)\n",
      "AFTER:  tensor([[ 0.2787, -0.0465, -0.3562,  0.2027, -0.1504],\n",
      "        [-0.0465,  0.5288,  0.2056, -0.3359, -0.1418],\n",
      "        [-0.3562,  0.2056,  0.2013, -0.3447, -0.2584],\n",
      "        [ 0.2027, -0.3359, -0.3447,  0.0609, -0.2272],\n",
      "        [-0.1504, -0.1418, -0.2584, -0.2272, -0.2602]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PairwiseParametrization(torch.nn.Module):\n",
    "    def forward(self, W):\n",
    "        # Construct a symmetric matrix with zero diagonal\n",
    "        # The weights are initialized to be non-squared, with 2 additional columns. We cut from two of these\n",
    "        # two vectors q and r, and then we compute w_diag as described in the paper.\n",
    "        # This procedure is done in order to easily distribute the mass in its spectrum through the values of q and r\n",
    "        W0 = W[:, :-2].triu(1)\n",
    "\n",
    "        W0 = W0 + W0.T\n",
    "\n",
    "        # Retrieve the `q` and `r` vectors from the last two columns\n",
    "        q = W[:, -2]\n",
    "        r = W[:, -1]\n",
    "        # Construct the main diagonal\n",
    "        w_diag = torch.diag(q * torch.sum(torch.abs(W0), 1) + r) \n",
    "\n",
    "        return W0 + w_diag\n",
    "\n",
    "layer = torch.nn.Linear(hidden_dimension + 2, hidden_dimension)\n",
    "print(\"BEFORE: \", layer.weight)\n",
    "# LAYER DEFINITION & SYMMETRY are now separated processes.\n",
    "parametrize.register_parametrization(layer, 'weight', PairwiseParametrization(), unsafe = True) \n",
    "# unsafe = True is used to change the tensor dimension with the re-parametrization.\n",
    "print(\"AFTER: \", layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
