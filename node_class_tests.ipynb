{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Parameter\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree, homophily\n",
    "\n",
    "from torch_geometric.datasets import WebKB, Planetoid\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GCNConv cell explains how to implement a customized MPNN, with MessagePassing class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages.\n",
    "\n",
    "        ''' self.propagate has to take as input at least 'edge_index' and 'x', then we can specify also other arguments,\n",
    "            like ciao. '''\n",
    "\n",
    "        return self.propagate(edge_index, ciao = 'ciao', size = (x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, x_j, edge_index, ciao, size):\n",
    "        # x_j has shape [E, out_channels]\n",
    "        # Step 3: Normalize node features.\n",
    "        ''' x_j contains the node features for row '''\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == 'inf'] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        print(\"1: \", (norm.view(-1, 1)*x_j).shape)\n",
    "        print(x_j.shape)\n",
    "        print(norm.view(-1, 1).shape)\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        print(\"2: \", aggr_out.shape)\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_texas = WebKB(root='/tmp/Texas', name='Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset_texas.x\n",
    "edges = dataset_texas.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  torch.Size([508, 5])\n",
      "torch.Size([508, 5])\n",
      "torch.Size([508, 1])\n",
      "2:  torch.Size([183, 5])\n"
     ]
    }
   ],
   "source": [
    "test_network = GCNConv(dataset_texas.num_features, dataset_texas.num_classes)\n",
    "t = test_network(x, edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized implementation for the SAGEConv GNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGECv(MessagePassing):\n",
    "    def __init__(self, input_dim, hidden_dim, project = False):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.l1 = torch.nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.l2 = torch.nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.project = project\n",
    "        if self.project:\n",
    "            self.l3 = torch.nn.Linear(input_dim, hidden_dim, bias = True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        if self.project:\n",
    "            x = F.relu(self.l3(x))\n",
    "\n",
    "        return self.propagate(edge_index, x = x)\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out is always the output after the aggregation, and x are the nodes to update.\n",
    "        aggr_out = self.l1(aggr_out)\n",
    "\n",
    "        out = self.l2(x) + self.l1(aggr_out)\n",
    "\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SAGECv(dataset_texas.num_features, dataset_texas.num_classes, project = True)\n",
    "\n",
    "t = s(x, edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a linear layer symmetric, info at [this link](https://pytorch.org/tutorials/intermediate/parametrizations.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,    2,   44],\n",
      "        [   0,    2, 3111],\n",
      "        [   0,    0,    4]])\n",
      "tensor([[   0,    2,   44],\n",
      "        [   0,    0, 3111],\n",
      "        [   0,    0,    0]])\n",
      "tensor([[   1,    2,   44],\n",
      "        [   2,    2, 3111],\n",
      "        [  44, 3111,    4]])\n"
     ]
    }
   ],
   "source": [
    "# Given a square matrix:\n",
    "matrix = torch.tensor([[1, 2, 44], [1, 2, 3111], [0, 1, 4]])\n",
    "# We can get its upper triangular part as \n",
    "print(matrix.triu(0))\n",
    "# and also its counter-version with zero diagonal terms\n",
    "print(matrix.triu(1))\n",
    "# Symmetric matrix is:\n",
    "print(matrix.triu(0) + matrix.triu(1).transpose(-1, -2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to parametrize the linear layers, but also to separate this process by the layer definition we should proceed by using the torch.nn.parametrize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  Parameter containing:\n",
      "tensor([[-0.0282, -0.2466,  0.1203, -0.0225, -0.2457],\n",
      "        [ 0.1340,  0.0759, -0.1351, -0.2224, -0.0830],\n",
      "        [-0.1430,  0.2537,  0.2474,  0.0577, -0.4088],\n",
      "        [ 0.3516, -0.1308,  0.4388, -0.1365, -0.2749],\n",
      "        [-0.1415,  0.2817,  0.4037,  0.0214, -0.0827]], requires_grad=True)\n",
      "AFTER:  tensor([[-0.0282, -0.2466,  0.1203, -0.0225, -0.2457],\n",
      "        [-0.2466,  0.0759, -0.1351, -0.2224, -0.0830],\n",
      "        [ 0.1203, -0.1351,  0.2474,  0.0577, -0.4088],\n",
      "        [-0.0225, -0.2224,  0.0577, -0.1365, -0.2749],\n",
      "        [-0.2457, -0.0830, -0.4088, -0.2749, -0.0827]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "class Symmetric(torch.nn.Module):\n",
    "    def forward(self, w):\n",
    "        # This class implements the method to define the symmetry in the squared matrices.\n",
    "        return w.triu(0) + w.triu(1).transpose(-1, -2)\n",
    "\n",
    "hidden_dimension = 5\n",
    "\n",
    "# Let's notice that we need to define squared layers\n",
    "layer = torch.nn.Linear(hidden_dimension, hidden_dimension)\n",
    "print(\"BEFORE: \", layer.weight)\n",
    "# LAYER DEFINITION & SYMMETRY are now separated processes.\n",
    "parametrize.register_parametrization(layer, 'weight', Symmetric())\n",
    "print(\"AFTER: \", layer.weight)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see now the symmetry in the GRAFF paper\n",
    "Here the parametrization is done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  Parameter containing:\n",
      "tensor([[-0.1944,  0.2896, -0.1541,  0.1468,  0.0238,  0.3736, -0.0285],\n",
      "        [-0.3449,  0.3421,  0.2561,  0.1855, -0.2920, -0.0639, -0.1927],\n",
      "        [ 0.0495,  0.0405, -0.2633,  0.1034, -0.1770,  0.1188, -0.0787],\n",
      "        [ 0.1491,  0.3262,  0.1329,  0.2281, -0.0370, -0.0072, -0.1882],\n",
      "        [ 0.0835,  0.1011, -0.1975, -0.1815,  0.2231, -0.1395,  0.0717]],\n",
      "       requires_grad=True)\n",
      "AFTER:  tensor([[ 0.2010,  0.2896, -0.1541,  0.1468,  0.0238],\n",
      "        [ 0.2896, -0.2581,  0.2561,  0.1855, -0.2920],\n",
      "        [-0.1541,  0.2561,  0.0034,  0.1034, -0.1770],\n",
      "        [ 0.1468,  0.1855,  0.1034, -0.1916, -0.0370],\n",
      "        [ 0.0238, -0.2920, -0.1770, -0.0370, -0.0022]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PairwiseParametrization(torch.nn.Module):\n",
    "    def forward(self, W):\n",
    "        # Construct a symmetric matrix with zero diagonal\n",
    "        # The weights are initialized to be non-squared, with 2 additional columns. We cut from two of these\n",
    "        # two vectors q and r, and then we compute w_diag as described in the paper.\n",
    "        # This procedure is done in order to easily distribute the mass in its spectrum through the values of q and r\n",
    "        W0 = W[:, :-2].triu(1)\n",
    "\n",
    "        W0 = W0 + W0.T\n",
    "\n",
    "        # Retrieve the `q` and `r` vectors from the last two columns\n",
    "        q = W[:, -2]\n",
    "        r = W[:, -1]\n",
    "        # Construct the main diagonal\n",
    "        w_diag = torch.diag(q * torch.sum(torch.abs(W0), 1) + r)\n",
    "\n",
    "        return W0 + w_diag\n",
    "\n",
    "\n",
    "layer = torch.nn.Linear(hidden_dimension + 2, hidden_dimension)\n",
    "print(\"BEFORE: \", layer.weight)\n",
    "# LAYER DEFINITION & SYMMETRY are now separated processes.\n",
    "parametrize.register_parametrization(\n",
    "    layer, 'weight', PairwiseParametrization(), unsafe=True)\n",
    "# unsafe = True is used to change the tensor dimension with the re-parametrization.\n",
    "print(\"AFTER: \", layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class External_W(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.empty((1, input_dim)))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.normal_(self.w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x * self.w behave like a diagonal matrix op., we multiply each row of x by the element-wise w\n",
    "        return x * self.w\n",
    "\n",
    "\n",
    "class Source_b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.beta = torch.nn.Parameter(torch.empty(1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.normal_(self.beta)\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.beta\n",
    "\n",
    "\n",
    "class PairwiseInteraction_w(nn.Module):\n",
    "    def __init__(self, input_dim, symmetry_type='1'):\n",
    "        super().__init__()\n",
    "        self.W = torch.nn.Linear(input_dim + 2, input_dim)\n",
    "\n",
    "        if symmetry_type == '1':\n",
    "            symmetry = PairwiseParametrization()\n",
    "        elif symmetry_type == '2':\n",
    "            symmetry = Symmetric()\n",
    "\n",
    "        parametrize.register_parametrization(\n",
    "            self.W, 'weight', symmetry, unsafe=True)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.W.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W(x)\n",
    "\n",
    "\n",
    "class GRAFFConv(MessagePassing):\n",
    "    def __init__(self, input_dim, symmetry_type='1', self_loops=True):\n",
    "        super().__init__(aggr='add')\n",
    "        self.in_dim = input_dim\n",
    "        self.self_loops = self_loops\n",
    "        self.external_w = External_W(self.in_dim)\n",
    "        self.beta = Source_b()\n",
    "        self.pairwise_W = PairwiseInteraction_w(\n",
    "            self.in_dim, symmetry_type=symmetry_type)\n",
    "\n",
    "    def forward(self, x, edge_index, x0):\n",
    "\n",
    "        # We set the source term, which corrensponds with the initial conditions of our system.\n",
    "\n",
    "        if self.self_loops:\n",
    "            edge_index, _ = add_self_loops(edge_index, num_nodes=x.shape[0])\n",
    "\n",
    "        out_p = self.pairwise_W(x)\n",
    "\n",
    "        out = self.propagate(edge_index, x=out_p)\n",
    "\n",
    "        out = out - (self.external_w(x) + self.beta(x0))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, edge_index, x):\n",
    "        # Does we need the degree of the row or from the columns?\n",
    "        # x_i are the columns indices, whereas x_j are the row indices\n",
    "        row, col = edge_index\n",
    "\n",
    "        # Degree is specified by the row (outgoing edges)\n",
    "        deg_matrix = degree(row, num_nodes=x.shape[0], dtype=x.dtype)\n",
    "        deg_inv = deg_matrix.pow(-0.5)\n",
    "        \n",
    "        deg_inv[deg_inv == float('inf')] = 0\n",
    "\n",
    "        denom_degree = deg_inv[row]*deg_inv[col]\n",
    "\n",
    "        # Each row of denom_degree multiplies (element-wise) the rows of x_j\n",
    "        return denom_degree.unsqueeze(-1) * x_i\n",
    "\n",
    "\n",
    "class PhysicsGNN(nn.Module):\n",
    "    def __init__(self, dataset, hidden_dim, num_layers, step = 0.1, symmetry_type='1', self_loops=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc = torch.nn.Linear(dataset.num_features, hidden_dim)\n",
    "        self.dec = torch.nn.Linear(hidden_dim, dataset.num_classes)\n",
    "\n",
    "        self.layers = [GRAFFConv(hidden_dim, symmetry_type=symmetry_type,\n",
    "                            self_loops=self_loops) for i in range(num_layers)]\n",
    "        self.step = step\n",
    "    #     self.reset_parameters()\n",
    "\n",
    "    # def reset_parameters(self):\n",
    "    #     self.enc.reset_parameters()\n",
    "    #     self.dec.reset_parameters()\n",
    "    #     for layer in self.layers:\n",
    "    #         layer.reset_parameters()\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        \n",
    "        x = enc_out = self.enc(x)\n",
    "\n",
    "        x0 = enc_out.clone()\n",
    "        for layer in self.layers:\n",
    "                \n",
    "            x = x + self.step*F.relu(layer(x, edge_index, x0))\n",
    "\n",
    "        output = self.dec(x)\n",
    "\n",
    "        return F.log_softmax(output, dim=1)\n",
    "        \n",
    "            \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = PhysicsGNN(dataset_texas, 512, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = g(dataset_texas.x, dataset_texas.edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
