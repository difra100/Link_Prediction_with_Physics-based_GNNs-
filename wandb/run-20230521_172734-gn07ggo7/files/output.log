
Epoch 63: 100%|██████████| 2/2 [00:00<00:00, 54.15it/s, loss=1.33, v_num=ggo7, Loss on train=1.290, Accuracy on train=0.678, Loss on test=1.420, Accuracy on test=0.525]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
  | Name  | Type          | Params
----------------------------------------
0 | model | PhysicsGNN_NC | 875 K
----------------------------------------
875 K     Trainable params
0         Non-trainable params
875 K     Total params
3.500     Total estimated model params size (MB)
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/peppe/Desktop/Università/magistrale/Thesis/Link_Prediction_with_Physics-based_GNNs-/Sweep/version_None/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

Epoch 97: 100%|██████████| 2/2 [00:00<00:00, 42.92it/s, loss=1.17, v_num=ggo7, Loss on train=1.140, Accuracy on train=0.713, Loss on test=1.350, Accuracy on test=0.525]
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.

Epoch 99: 100%|██████████| 2/2 [00:00<00:00, 37.32it/s, loss=1.17, v_num=ggo7, Loss on train=1.130, Accuracy on train=0.713, Loss on test=1.340, Accuracy on test=0.525]