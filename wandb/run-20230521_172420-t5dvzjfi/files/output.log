GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
  | Name  | Type          | Params
----------------------------------------
0 | model | PhysicsGNN_NC | 109 K
----------------------------------------
109 K     Trainable params
0         Non-trainable params
109 K     Total params
0.438     Total estimated model params size (MB)
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/peppe/Desktop/Università/magistrale/Thesis/Link_Prediction_with_Physics-based_GNNs-/Sweep/version_None/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(



Epoch 326: 100%|██████████| 2/2 [00:00<00:00, 95.19it/s, loss=0.0548, v_num=zjfi, Loss on train=0.0523, Accuracy on train=1.000, Loss on test=0.744, Accuracy on test=0.780]


Epoch 526: 100%|██████████| 2/2 [00:00<00:00, 80.06it/s, loss=0.0247, v_num=zjfi, Loss on train=0.0241, Accuracy on train=1.000, Loss on test=0.728, Accuracy on test=0.763]





Epoch 795:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.0146, v_num=zjfi, Loss on train=0.0144, Accuracy on train=1.000, Loss on test=0.716, Accuracy on test=0.780]
Epoch 841: 100%|██████████| 2/2 [00:00<00:00, 88.16it/s, loss=0.0138, v_num=zjfi, Loss on train=0.0136, Accuracy on train=1.000, Loss on test=0.714, Accuracy on test=0.780]



Epoch 999: 100%|██████████| 2/2 [00:00<00:00, 58.25it/s, loss=0.0117, v_num=zjfi, Loss on train=0.0116, Accuracy on train=1.000, Loss on test=0.706, Accuracy on test=0.780]
/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.
  rank_zero_deprecation(