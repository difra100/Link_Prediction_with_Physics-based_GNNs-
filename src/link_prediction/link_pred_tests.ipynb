{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction features initialized.....\n"
     ]
    }
   ],
   "source": [
    "######## IMPORT EXTERNAL FILES ###########\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "######### IMPORT INTERNAL FILES ###########\n",
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "from GRAFF import *\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convert_to_networkx(graph, n_sample=None):\n",
    "\n",
    "    g = to_networkx(graph, node_attrs=[\"x\"])\n",
    "    y = graph.y.numpy()\n",
    "\n",
    "    if n_sample is not None:\n",
    "        sampled_nodes = random.sample(g.nodes, n_sample)\n",
    "        g = g.subgraph(sampled_nodes)\n",
    "        y = y[sampled_nodes]\n",
    "\n",
    "    return g, y\n",
    "\n",
    "\n",
    "def plot_graph(g, y):\n",
    "\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    nx.draw_spring(g, node_size=30, arrows=False, node_color=y)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g, y = convert_to_networkx(dataset[0])\n",
    "# plot_graph(g, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# final_dataset = train_test_split_edges(dataset[0], val_ratio = 0.1, test_ratio= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indices(dataset, split_idx):\n",
    "#     ''' According to the dataset, and the specified splitting (e.g. in Geom-GCN there are 10 splits) \n",
    "#         We identify the indices. \n",
    "\n",
    "#         args:\n",
    "#           - dataset: torch-geometric data type,\n",
    "#           - split_idx: in the Geom-GCN implementations the available splittings are from 0-9    \n",
    "        \n",
    "#         output:\n",
    "#           - (train_indices, val_indices, test_indices):\n",
    "#                  indices that corrensponds to the whole graph. \n",
    "    \n",
    "#     '''\n",
    "\n",
    "#     train_idx = dataset.train_mask[:, split_idx]\n",
    "#     val_idx = dataset.val_mask[:, split_idx]\n",
    "#     test_idx = dataset.test_mask[:, split_idx]\n",
    "\n",
    "#     train_indices = torch.nonzero(train_idx)\n",
    "#     val_indices = torch.nonzero(val_idx)\n",
    "#     test_indices = torch.nonzero(test_idx)\n",
    "\n",
    "#     return train_indices.squeeze(1), val_indices.squeeze(1), test_indices.squeeze(1)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# final = train_test_split_edges(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleLP(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,  train_set, val_set, test_set, mode, batch_size):\n",
    "\n",
    "        self.mode = mode  # \"hp\" or \"test\"\n",
    "        self.batch_size = batch_size\n",
    "        self.train_set, self.val_set, self.test_set = train_set, val_set, test_set\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit':\n",
    "\n",
    "            # edge_index are the message passing edges,\n",
    "            # edge_label_index are the supervision edges.\n",
    "            if self.train_set.pos_edge_label_index.shape[1] < self.train_set.edge_index.shape[1]:\n",
    "                pos_mask_edge = self.train_set.pos_edge_label_index.shape[1]\n",
    "\n",
    "                self.train_set.edge_index = self.train_set.edge_index[:, pos_mask_edge:]\n",
    "            else:\n",
    "                self.train_set.pos_edge_label_index = self.train_set.edge_index[:, :self.train_set.edge_index.shape[1] // 2]\n",
    "                self.train_set.neg_edge_label_index = self.train_set.neg_edge_label_index[:, :self.train_set.edge_index.shape[1] // 2]\n",
    "\n",
    "                self.train_set.edge_index = self.train_set.edge_index[:, self.train_set.edge_index.shape[1] // 2:]\n",
    "\n",
    "                \n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs):\n",
    "        return DataLoader([self.train_set], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs):\n",
    "        if self.mode == 'hp':\n",
    "            return DataLoader([self.val_set], batch_size=batch_size, shuffle=False)\n",
    "        elif self.mode == 'test':\n",
    "            return DataLoader([self.test_set], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'hp'\n",
    "save = True\n",
    "if save:\n",
    "    transform = RandomLinkSplit(num_val = 0.1, num_test = 0.1, is_undirected=True if dataset_name != 'Texas' else False, split_labels= True, neg_sampling_ratio=200)\n",
    "\n",
    "    # Edges are divided into three sets\n",
    "    train_data, val_data, test_data = transform(dataset[0])\n",
    "\n",
    "    # Negative edges are extracted\n",
    "    torch.save(train_data, dataset_name + \"/train_data.pt\")\n",
    "    torch.save(val_data, dataset_name + \"/val_data.pt\")\n",
    "    torch.save(test_data, dataset_name + \"/test_data.pt\")\n",
    "load = True\n",
    "if load:\n",
    "    train_data = torch.load(dataset_name + \"/train_data.pt\")\n",
    "    val_data = torch.load(dataset_name + \"/val_data.pt\")\n",
    "    test_data = torch.load(dataset_name + \"/test_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM = DataModuleLP(train_data.clone(), val_data.clone(), test_data.clone(), mode = 'hp', batch_size = batch_size)\n",
    "DM.setup('fit')\n",
    "DM.setup('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708, 10], val_mask=[2708, 10], test_mask=[2708, 10], pos_edge_label=[4224], pos_edge_label_index=[2, 4224], neg_edge_label=[844800], neg_edge_label_index=[2, 844800])\n",
      "Data(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708, 10], val_mask=[2708, 10], test_mask=[2708, 10], pos_edge_label=[527], pos_edge_label_index=[2, 527], neg_edge_label=[105400], neg_edge_label_index=[2, 105400])\n",
      "Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708, 10], val_mask=[2708, 10], test_mask=[2708, 10], pos_edge_label=[527], pos_edge_label_index=[2, 527], neg_edge_label=[105400], neg_edge_label_index=[2, 105400])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[2708, 1433], edge_index=[2, 4224], y=[2708], train_mask=[2708, 10], val_mask=[2708, 10], test_mask=[2708, 10], pos_edge_label=[4224], pos_edge_label_index=[2, 4224], neg_edge_label=[844800], neg_edge_label_index=[2, 844800], batch=[2708], ptr=[2])\n",
      "DataBatch(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708, 10], val_mask=[2708, 10], test_mask=[2708, 10], pos_edge_label=[527], pos_edge_label_index=[2, 527], neg_edge_label=[105400], neg_edge_label_index=[2, 105400], batch=[2708], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "for i in DM.train_dataloader():\n",
    "    print(i)\n",
    "for i in DM.val_dataloader():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers = 0, bias = False, dropout= 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        if self.num_layers != 0:\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(input_dim, output_dim, bias = bias))\n",
    "            for layer in range(self.num_layers):\n",
    "                layers.append(nn.Linear(output_dim, output_dim, bias = bias))\n",
    "            layers.append(nn.Linear(output_dim, 1, bias = bias))\n",
    "        \n",
    "            self.layers = nn.Sequential(*layers)\n",
    "            self.dropout = dropout\n",
    "             \n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "        \n",
    "    def forward(self, x_i, x_j, training = False):\n",
    "        \n",
    "        out = x_i * x_j \n",
    "        if self.num_layers != 0:\n",
    "            for layer in self.layers:\n",
    "                out = layer(out)\n",
    "                out = F.relu(out)\n",
    "                out = F.dropout(out, p = self.dropout, training = training)\n",
    "        out = out.sum(dim = -1)\n",
    "        print(out)\n",
    "        return torch.sigmoid(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PhysicsGNN_LP(nn.Module):\n",
    "    def __init__(self, dataset, hidden_dim, output_dim, num_layers, num_layers_mlp, link_bias, dropout, step=0.1, symmetry_type='1', self_loops=False, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc = torch.nn.Linear(\n",
    "            dataset.num_features, hidden_dim, bias=False)\n",
    "\n",
    "        self.external_w = External_W(hidden_dim, device=device)\n",
    "        self.source_b = Source_b(device=device)\n",
    "        self.pairwise_w = PairwiseInteraction_w(\n",
    "            hidden_dim, symmetry_type=symmetry_type, device=device)\n",
    "\n",
    "        self.layers = [GRAFFConv(self.external_w, self.source_b, self.pairwise_w,\n",
    "                                 self_loops=self_loops) for i in range(num_layers)]\n",
    "\n",
    "        self.step = step\n",
    "        self.link_pred = LinkPredictor(\n",
    "            hidden_dim, output_dim, num_layers_mlp, link_bias, dropout)\n",
    "        self.reset_parameters()\n",
    "        self.to(device)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.enc.reset_parameters()\n",
    "        self.external_w.reset_parameters()\n",
    "        self.source_b.reset_parameters()\n",
    "        self.pairwise_w.reset_parameters()\n",
    "        self.link_pred.reset_parameters()\n",
    "\n",
    "    def forward(self, data, train=True):\n",
    "\n",
    "        if train:\n",
    "            x, edge_index = data.x.clone(), data.pos_forward_pass.clone()\n",
    "        else:\n",
    "            x, edge_index = data.x.clone(), data.edge_index.clone()\n",
    "\n",
    "        x = enc_out = self.enc(x)\n",
    "\n",
    "        x0 = enc_out.clone()\n",
    "        for layer in self.layers:\n",
    "\n",
    "            x = x + self.step*F.relu(layer(x, edge_index, x0))\n",
    "\n",
    "        if train:\n",
    "            pos_edge = data.pos_masked_edges.clone()\n",
    "        else:\n",
    "            pos_edge = data.edge_label_index.clone()\n",
    "\n",
    "        neg_edge = data.neg_edges.clone()\n",
    "        pos_pred = self.link_pred(\n",
    "            x[pos_edge[0]], x[pos_edge[1]], training=train)\n",
    "        neg_pred = self.link_pred(\n",
    "            x[neg_edge[0]], x[neg_edge[1]], training=train)\n",
    "\n",
    "        return pos_pred, neg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 64\n",
    "mlp_layer = 2\n",
    "link_bias = False\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PG = PhysicsGNN_LP(dataset, hidden_dim, output_dim, num_layers, mlp_layer, link_bias, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.3387e-06, 7.6268e-04, 2.8153e-04, 0.0000e+00, 2.1349e-04, 8.6843e-04,\n",
      "        1.0140e-03, 5.3435e-04, 6.4876e-04, 0.0000e+00, 1.3798e-03, 1.1496e-04,\n",
      "        0.0000e+00, 2.6795e-04, 6.6629e-04, 0.0000e+00, 6.1495e-04, 0.0000e+00,\n",
      "        5.2026e-04, 2.1459e-04, 4.6524e-04, 1.2086e-03, 1.2082e-03, 1.3040e-03,\n",
      "        3.1008e-04, 1.2190e-04, 0.0000e+00, 9.2872e-04, 8.1915e-04, 5.3745e-04,\n",
      "        1.9134e-03, 1.7363e-04, 3.2125e-04, 0.0000e+00, 0.0000e+00, 9.0923e-05,\n",
      "        0.0000e+00, 3.2903e-03, 1.7395e-03, 0.0000e+00, 0.0000e+00, 9.6303e-04,\n",
      "        4.3483e-04, 0.0000e+00, 3.8775e-04, 5.3147e-04, 0.0000e+00, 1.5167e-03,\n",
      "        9.7916e-04, 6.8913e-05, 0.0000e+00, 9.5810e-04, 1.4172e-03, 0.0000e+00,\n",
      "        8.2258e-04, 1.6605e-03, 4.6283e-04, 0.0000e+00, 1.0299e-03, 6.2316e-04,\n",
      "        2.0211e-05, 0.0000e+00, 8.4758e-04, 1.4357e-03, 7.6352e-04, 5.0854e-04,\n",
      "        5.2834e-04, 5.9502e-04, 0.0000e+00], grad_fn=<SumBackward1>)\n",
      "tensor([2.1905e-04, 0.0000e+00, 1.8350e-03, 0.0000e+00, 4.2313e-04, 6.7575e-04,\n",
      "        1.1985e-03, 4.5082e-04, 1.7964e-04, 1.1658e-03, 1.6634e-04, 4.8767e-05,\n",
      "        8.4341e-04, 7.8770e-04, 4.1576e-04, 6.4250e-04, 6.4163e-04, 6.7761e-04,\n",
      "        7.0573e-04, 5.9372e-04, 6.3687e-04, 2.0342e-04, 5.7112e-04, 4.3769e-05,\n",
      "        1.5188e-04, 3.0307e-05, 8.4190e-04, 0.0000e+00, 0.0000e+00, 4.7372e-04,\n",
      "        3.7164e-04, 7.4929e-04, 3.6341e-04, 2.3543e-04, 2.1789e-03, 1.2965e-03,\n",
      "        0.0000e+00, 7.9778e-04, 0.0000e+00, 3.2724e-04, 1.2681e-04, 2.6722e-03,\n",
      "        0.0000e+00, 8.2968e-04, 0.0000e+00, 1.6762e-03, 1.3261e-03, 2.8103e-04,\n",
      "        2.6585e-03, 1.9848e-04, 2.8033e-04, 0.0000e+00, 0.0000e+00, 2.1637e-04,\n",
      "        8.2897e-04, 0.0000e+00, 3.9450e-04, 4.4169e-04, 5.2750e-04, 9.3517e-04,\n",
      "        6.9860e-04, 9.9553e-04, 4.4825e-04, 6.9175e-04, 1.0085e-03, 1.3905e-03,\n",
      "        1.6973e-03, 2.7768e-04, 7.7625e-04], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out = PG(train_data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(pred_positive, 100 if pred_positive.shape[0] >= 100 else pred_positive.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100 if pred_positive.shape[0] >= 100 else pred_positive.shape[0]\n",
    "hit_count = 0\n",
    "positive_indices = list(range(pred_positive.shape[0]))\n",
    "top_k_predictions = torch.topk(pred_positive, top_k).indices\n",
    "for i in range(len(positive_indices)):\n",
    "    if positive_indices[i] in top_k_predictions:\n",
    "        hit_count += 1\n",
    "\n",
    "hit_ratio = hit_count / len(positive_indices)\n",
    "hit_ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to repeat the experiments? \n",
    "# What are the splittings? If i do my own splittings should i repeat the experiments? \n",
    "# Message passing questions.........."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
