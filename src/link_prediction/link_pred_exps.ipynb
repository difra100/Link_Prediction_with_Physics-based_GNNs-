{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the experiments\n",
    "In this notebook are contained the following features:\n",
    "* GRAFF + Link prediction,\n",
    "\n",
    "The main tools that have been exploited are [PyTorch](https://pytorch.org/) (1.13.0), [PyTorch-Lightning](https://www.pytorchlightning.ai/index.html) (1.5.10), [Pytorch-geometric](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) (2.3.0) and [Weights & Biases](https://wandb.ai/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements to run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# !pip install pytorch-lightning==1.5.10\n",
    "# !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "# !pip install torch_geometric\n",
    "# !pip install wandb\n",
    "# !pip install ogb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction features initialized.....\n"
     ]
    }
   ],
   "source": [
    "######## IMPORT EXTERNAL FILES ###########\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import wandb\n",
    "######### IMPORT INTERNAL FILES ###########\n",
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "from GRAFF import *\n",
    "from config import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdifra00\u001b[0m (\u001b[33mdeepl_wizards\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = 1 if device == 'cuda' else 0\n",
    "\n",
    "if wb:\n",
    "    wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning DataModule (Link Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleLP(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,  train_set, val_set, test_set, mode, batch_size):\n",
    "\n",
    "        self.mode = mode  # \"hp\" or \"test\"\n",
    "        self.batch_size = batch_size\n",
    "        self.train_set, self.val_set, self.test_set = train_set, val_set, test_set\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit':\n",
    "\n",
    "            # edge_index are the message passing edges,\n",
    "            # edge_label_index are the supervision edges.\n",
    "            if self.train_set.pos_edge_label_index.shape[1] < self.train_set.edge_index.shape[1]:\n",
    "                pos_mask_edge = self.train_set.pos_edge_label_index.shape[1]\n",
    "\n",
    "                self.train_set.edge_index = self.train_set.edge_index[:,\n",
    "                                                                      pos_mask_edge:]\n",
    "            else:\n",
    "                self.train_set.pos_edge_label_index = self.train_set.edge_index[:,\n",
    "                                                                                :self.train_set.edge_index.shape[1] // 2]\n",
    "                self.train_set.neg_edge_label_index = self.train_set.neg_edge_label_index[\n",
    "                    :, :self.train_set.edge_index.shape[1] // 2]\n",
    "\n",
    "                self.train_set.edge_index = self.train_set.edge_index[:,\n",
    "                                                                      self.train_set.edge_index.shape[1] // 2:]\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs):\n",
    "        return DataLoader([self.train_set], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs):\n",
    "        if self.mode == 'hp':\n",
    "            return DataLoader([self.val_set], batch_size=batch_size, shuffle=False)\n",
    "        elif self.mode == 'test':\n",
    "            return DataLoader([self.test_set], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(dataset_name + \"/train_data.pt\")\n",
    "val_data = torch.load(dataset_name + \"/val_data.pt\")\n",
    "test_data = torch.load(dataset_name + \"/test_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data)\n",
    "# print(val_data)\n",
    "# print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'hp'  # hp: Hyperparameter selection mode\n",
    "sweep = True\n",
    "dataM = DataModuleLP(train_data.clone(), val_data.clone(), test_data.clone(), mode = mode, batch_size = batch_size)\n",
    "dataM.setup(stage='fit')\n",
    "dataM.setup(stage='test') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], edge_index=[2, 131], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], pos_edge_label=[261], pos_edge_label_index=[2, 130], neg_edge_label=[26499], neg_edge_label_index=[2, 130])\n",
      "Data(x=[183, 1703], edge_index=[2, 261], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], pos_edge_label=[32], pos_edge_label_index=[2, 32], neg_edge_label=[3248], neg_edge_label_index=[2, 3248])\n",
      "Data(x=[183, 1703], edge_index=[2, 293], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], pos_edge_label=[32], pos_edge_label_index=[2, 32], neg_edge_label=[3250], neg_edge_label_index=[2, 3250])\n"
     ]
    }
   ],
   "source": [
    "print(dataM.train_set)\n",
    "print(dataM.val_set)\n",
    "print(dataM.test_set)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Get_Metrics(Callback):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "\n",
    "        # Compute the metrics\n",
    "        train_loss = sum(\n",
    "            pl_module.train_prop['loss']) / len(pl_module.train_prop['loss'])\n",
    "        train_acc100 = sum(\n",
    "            pl_module.train_prop['HR@100']) / len(pl_module.train_prop['HR@100'])\n",
    "        # train_acc20 = sum(\n",
    "        #     pl_module.train_prop['HR@20']) / len(pl_module.train_prop['HR@20'])\n",
    "        # train_acc1 = sum(\n",
    "        #     pl_module.train_prop['HR@1']) / len(pl_module.train_prop['HR@1'])\n",
    "        test_loss = sum(\n",
    "            pl_module.test_prop['loss']) / len(pl_module.test_prop['loss'])\n",
    "        \n",
    "        test_acc100 = sum(pl_module.test_prop['HR@100']) / \\\n",
    "            len(pl_module.test_prop['HR@100'])\n",
    "        # test_acc20 = sum(pl_module.test_prop['HR@20']) / \\\n",
    "        #     len(pl_module.test_prop['HR@20'])\n",
    "        # test_acc1 = sum(pl_module.test_prop['HR@1']) / \\\n",
    "        #     len(pl_module.test_prop['HR@1'])\n",
    "\n",
    "        # Log the metrics\n",
    "        pl_module.log(name='Loss on train', value=train_loss,\n",
    "                      on_epoch=True, prog_bar=True, logger=True)\n",
    "        pl_module.log(name='Loss on test', value=test_loss,\n",
    "                      on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        pl_module.log(name='HR@100 on train', value=train_acc100,\n",
    "                      on_epoch=True, prog_bar=True, logger=True)\n",
    "        pl_module.log(name='HR@100 on test', value=test_acc100,\n",
    "                            on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        # pl_module.log(name='HR@20 on train', value=train_acc20,\n",
    "        #               on_epoch=True, prog_bar=True, logger=True)\n",
    "        # pl_module.log(name='HR@20 on test', value=test_acc20,\n",
    "        #               on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        \n",
    "        # pl_module.log(name='HR@1 on train', value=train_acc1,\n",
    "        #               on_epoch=True, prog_bar=True, logger=True)\n",
    "        # pl_module.log(name='HR@1 on test', value=test_acc1,\n",
    "        #               on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Re-initialize the metrics\n",
    "        pl_module.train_prop['loss'] = []\n",
    "        pl_module.train_prop['HR@100'] = []\n",
    "        pl_module.train_prop['HR@20'] = []\n",
    "        pl_module.train_prop['HR@1'] = []\n",
    "\n",
    "        pl_module.test_prop['loss'] = []\n",
    "        pl_module.test_prop['HR@100'] = []\n",
    "        pl_module.test_prop['HR@20'] = []\n",
    "        pl_module.test_prop['HR@1'] = []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning Training Module (Node Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, predictor, lr, wd):\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "        self.predictor = predictor.to(device)\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "\n",
    "        self.train_prop = {'loss': [], 'HR@100': [], 'HR@20': [], 'HR@1': []}\n",
    "        self.test_prop = {'loss': [], 'HR@100': [], 'HR@20': [], 'HR@1': []}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        out = self.model(batch)\n",
    "\n",
    "        pos_edge = batch.pos_edge_label_index\n",
    "\n",
    "        pos_pred = self.predictor(\n",
    "            out[pos_edge[0]], out[pos_edge[1]], training=True)\n",
    "\n",
    "        neg_edge = batch.neg_edge_label_index\n",
    "\n",
    "        neg_pred = self.predictor(\n",
    "            out[neg_edge[0]], out[neg_edge[1]], training=True)\n",
    "\n",
    "        loss = -torch.log(pos_pred + 1e-15).mean() - \\\n",
    "            torch.log(1 - neg_pred[:pos_pred.shape[0]] + 1e-15).mean()\n",
    "\n",
    "        acc100 = evaluate(pos_pred, neg_pred[pos_pred.shape[0]: 2*pos_pred.shape[0]], k=100)\n",
    "        # acc20 = evaluate(pos_pred, neg_pred, k = 20)\n",
    "        # acc1 = evaluate(pos_pred, neg_pred, k = 1)\n",
    "\n",
    "        self.train_prop['loss'].append(loss)\n",
    "        self.train_prop['HR@100'].append(acc100)\n",
    "        # self.train_prop['HR@20'].append(acc20)\n",
    "        # self.train_prop['HR@1'].append(acc1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        out = self.model(batch)\n",
    "\n",
    "        pos_edge = batch.pos_edge_label_index\n",
    "\n",
    "        # training is for dropout\n",
    "        pos_pred = self.predictor(\n",
    "            out[pos_edge[0]], out[pos_edge[1]], training=False)\n",
    "\n",
    "        neg_edge = batch.neg_edge_label_index\n",
    "\n",
    "        # training is for dropout\n",
    "        neg_pred = self.predictor(\n",
    "            out[neg_edge[0]], out[neg_edge[1]], training=False)\n",
    "\n",
    "        loss = -torch.log(pos_pred + 1e-15).mean() - \\\n",
    "            torch.log(1 - neg_pred[:pos_pred.shape[0]] + 1e-15).mean()\n",
    "\n",
    "        acc100 = evaluate(pos_pred, neg_pred[pos_pred.shape[0]: pos_pred.shape[0]*2], k=100)\n",
    "        # acc20 = evaluate(pos_pred, neg_pred, k = 20)\n",
    "        # acc1 = evaluate(pos_pred, neg_pred, k = 1)\n",
    "\n",
    "        self.test_prop['loss'].append(loss)\n",
    "        self.test_prop['HR@100'].append(acc100)\n",
    "        # self.test_prop['HR@20'].append(acc20)\n",
    "        # self.test_prop['HR@1'].append(acc1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            list(self.model.parameters()) + list(self.predictor.parameters()), lr=self.lr, weight_decay=self.wd)\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "def evaluate(pos_pred, neg_pred, k=100):\n",
    "    n_indices = pos_pred.shape[0]\n",
    "    hr = 0\n",
    "\n",
    "    k = min(neg_pred.shape[0]+1, k)\n",
    "\n",
    "    for pos_idx in range(n_indices):\n",
    "        pos = pos_pred[pos_idx].unsqueeze(0)\n",
    "\n",
    "        # Checking if the predictions are the same over all the negative distribution\n",
    "        if round(pos.item(), 4) == round(torch.mean(neg_pred).item(), 4) and round(pos.item(), 4) == round(torch.min(neg_pred).item(), 4) and \\\n",
    "                round(pos.item(), 4) == round(torch.max(neg_pred).item(), 4):\n",
    "            continue\n",
    "        tot_tensor = torch.cat((neg_pred, pos), dim=0)\n",
    "        scores_idx = torch.topk(tot_tensor.squeeze(1), k).indices\n",
    "\n",
    "        # Check if the positive is in the top100. Positive is marked by the neg_pred.shape[0]\n",
    "        if neg_pred.shape[0] in scores_idx:\n",
    "\n",
    "            hr += 1\n",
    "    return hr/n_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hp enables a grid search on a wide set of hyperparameters.\n",
    "if not sweep or mode == 'test':\n",
    "    model = PhysicsGNN_LP(dataset, hidden_dim, num_layers, step = step)\n",
    "    # model = GNNStack(dataset.x.shape[1], hidden_dim, hidden_dim, num_layers, dropout, emb=True)\n",
    "    predictor = LinkPredictor(\n",
    "        hidden_dim, output_dim, mlp_layer, link_bias, dropout, device=device)\n",
    "    # predictor = LinkPredictor(\n",
    "    #      hidden_dim, hidden_dim, 1, num_layers,\n",
    "    #              dropout)\n",
    "    \n",
    "    pl_training_module = TrainingModule(model, predictor, lr, wd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'HR@100 on test'},\n",
      " 'parameters': {'dropout': {'values': [0, 0.2, 0.3, 0.4]},\n",
      "                'hidden_dim': {'values': [32, 64, 128, 256]},\n",
      "                'link_bias': {'values': [True, False]},\n",
      "                'lr': {'values': [0.01, 0.001, 0.0001]},\n",
      "                'mlp_layer': {'values': [0, 1, 2, 3]},\n",
      "                'num_layers': {'values': [1, 2, 3]},\n",
      "                'output_dim': {'values': [16, 32, 64]},\n",
      "                'step': {'values': [0.1, 0.2, 0.3]},\n",
      "                'wd': {'values': [0, 0.01, 0.001, 1e-06]}}}\n",
      "Create sweep with ID: h0tq9d5j\n",
      "Sweep URL: https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/sweeps/h0tq9d5j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j2g19xbx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_bias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp_layer: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dim: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstep: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twd: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/peppe/Desktop/Università/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/wandb/run-20230602_193424-j2g19xbx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/runs/j2g19xbx\" target=\"_blank\">resilient-sweep-1</a></strong> to <a href=\"https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/sweeps/h0tq9d5j\" target=\"_blank\">https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/sweeps/h0tq9d5j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | model     | GNNStack      | 115 K \n",
      "1 | predictor | LinkPredictor | 3.2 K \n",
      "--------------------------------------------\n",
      "118 K     Trainable params\n",
      "0         Non-trainable params\n",
      "118 K     Total params\n",
      "0.474     Total estimated model params size (MB)\n",
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/peppe/Desktop/Università/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/Sweep_LinkPred/version_None/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 80.15it/s, loss=1.4, v_num=9xbx, Loss on train=1.400, Loss on test=1.400, HR@100 on train=1.000, HR@100 on test=0.500]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 2/2 [00:00<00:00, 76.64it/s, loss=0.83, v_num=9xbx, Loss on train=0.658, Loss on test=1.420, HR@100 on train=1.000, HR@100 on test=1.000]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>HR@100 on test</td><td>▄███▂▁▁▂████████████████████████████████</td></tr><tr><td>HR@100 on train</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Loss on test</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▄▃▂▁▁▁▂▄▂▄▆█▇</td></tr><tr><td>Loss on train</td><td>█████████████████████████▇▇▇▇▆▄▄▃▃▂▂▂▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>HR@100 on test</td><td>1.0</td></tr><tr><td>HR@100 on train</td><td>1.0</td></tr><tr><td>Loss on test</td><td>1.41736</td></tr><tr><td>Loss on train</td><td>0.65802</td></tr><tr><td>epoch</td><td>76</td></tr><tr><td>trainer/global_step</td><td>76</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">resilient-sweep-1</strong>: <a href=\"https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/runs/j2g19xbx\" target=\"_blank\">https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/runs/j2g19xbx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230602_193424-j2g19xbx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xepbr9fs with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_bias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp_layer: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dim: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstep: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twd: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/peppe/Desktop/Università/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/wandb/run-20230602_193435-xepbr9fs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/runs/xepbr9fs\" target=\"_blank\">zany-sweep-2</a></strong> to <a href=\"https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/sweeps/h0tq9d5j\" target=\"_blank\">https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/sweeps/h0tq9d5j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | model     | GNNStack      | 534 K \n",
      "1 | predictor | LinkPredictor | 49.7 K\n",
      "--------------------------------------------\n",
      "584 K     Trainable params\n",
      "0         Non-trainable params\n",
      "584 K     Total params\n",
      "2.338     Total estimated model params size (MB)\n",
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/peppe/Desktop/Università/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/Sweep_LinkPred/version_None/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 76.45it/s, loss=1.39, v_num=r9fs, Loss on train=1.390, Loss on test=1.390, HR@100 on train=1.000, HR@100 on test=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 2/2 [00:00<00:00, 60.61it/s, loss=0.964, v_num=r9fs, Loss on train=0.685, Loss on test=3.230, HR@100 on train=1.000, HR@100 on test=1.000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ZMQDisplayPublisher' object has no attribute '_orig_publish'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/peppe/Desktop/Università/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/link_pred_exps.ipynb Cell 20\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/peppe/Desktop/Universit%C3%A0/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/link_pred_exps.ipynb#X25sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m sweep_id \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39msweep(sweep_config, project\u001b[39m=\u001b[39mproject_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/peppe/Desktop/Universit%C3%A0/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/link_pred_exps.ipynb#X25sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m wandb\u001b[39m.\u001b[39magent(sweep_id, sweep_train, count\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/peppe/Desktop/Universit%C3%A0/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/link_pred_exps.ipynb#X25sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m wandb\u001b[39m.\u001b[39;49mfinish()\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:3652\u001b[0m, in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   3642\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Marks a run as finished, and finishes uploading all data.\u001b[39;00m\n\u001b[1;32m   3643\u001b[0m \n\u001b[1;32m   3644\u001b[0m \u001b[39mThis is used when creating multiple runs in the same process.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3649\u001b[0m \u001b[39m    quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   3650\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3651\u001b[0m \u001b[39mif\u001b[39;00m wandb\u001b[39m.\u001b[39mrun:\n\u001b[0;32m-> 3652\u001b[0m     wandb\u001b[39m.\u001b[39;49mrun\u001b[39m.\u001b[39;49mfinish(exit_code\u001b[39m=\u001b[39;49mexit_code, quiet\u001b[39m=\u001b[39;49mquiet)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:292\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m         wandb\u001b[39m.\u001b[39mtermwarn(message, repeat\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    290\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mDummy()\n\u001b[0;32m--> 292\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:255\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    254\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_is_attaching \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 255\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1744\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[39m@_run_decorator\u001b[39m\u001b[39m.\u001b[39m_noop\n\u001b[1;32m   1731\u001b[0m \u001b[39m@_run_decorator\u001b[39m\u001b[39m.\u001b[39m_attach\n\u001b[1;32m   1732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish\u001b[39m(\n\u001b[1;32m   1733\u001b[0m     \u001b[39mself\u001b[39m, exit_code: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, quiet: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1735\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Marks a run as finished, and finishes uploading all data.\u001b[39;00m\n\u001b[1;32m   1736\u001b[0m \n\u001b[1;32m   1737\u001b[0m \u001b[39m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_finish(exit_code, quiet)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1757\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown_hooks:\n\u001b[1;32m   1756\u001b[0m     \u001b[39mif\u001b[39;00m hook\u001b[39m.\u001b[39mstage \u001b[39m==\u001b[39m TeardownStage\u001b[39m.\u001b[39mEARLY:\n\u001b[0;32m-> 1757\u001b[0m         hook\u001b[39m.\u001b[39;49mcall()\n\u001b[1;32m   1759\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_atexit_cleanup(exit_code\u001b[39m=\u001b[39mexit_code)\n\u001b[1;32m   1760\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wl \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wl\u001b[39m.\u001b[39m_global_run_stack) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/my_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:416\u001b[0m, in \u001b[0;36m_WandbInit._jupyter_teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_pause_backend\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m hook\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m:\n\u001b[1;32m    415\u001b[0m         ipython\u001b[39m.\u001b[39mevents\u001b[39m.\u001b[39munregister(\u001b[39m\"\u001b[39m\u001b[39mpost_run_cell\u001b[39m\u001b[39m\"\u001b[39m, hook)\n\u001b[0;32m--> 416\u001b[0m ipython\u001b[39m.\u001b[39mdisplay_pub\u001b[39m.\u001b[39mpublish \u001b[39m=\u001b[39m ipython\u001b[39m.\u001b[39;49mdisplay_pub\u001b[39m.\u001b[39;49m_orig_publish\n\u001b[1;32m    417\u001b[0m \u001b[39mdel\u001b[39;00m ipython\u001b[39m.\u001b[39mdisplay_pub\u001b[39m.\u001b[39m_orig_publish\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>HR@100 on test</td><td>▁▁█▂████████████████████████████████████</td></tr><tr><td>HR@100 on train</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Loss on test</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄▅█</td></tr><tr><td>Loss on train</td><td>█████████████████████████▇▇▇▇▆▆▅▅▅▄▃▃▂▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>HR@100 on test</td><td>1.0</td></tr><tr><td>HR@100 on train</td><td>1.0</td></tr><tr><td>Loss on test</td><td>3.22702</td></tr><tr><td>Loss on train</td><td>0.68528</td></tr><tr><td>epoch</td><td>72</td></tr><tr><td>trainer/global_step</td><td>72</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zany-sweep-2</strong>: <a href=\"https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/runs/xepbr9fs\" target=\"_blank\">https://wandb.ai/deepl_wizards/Link%20Prediction%20with%20PBGNN/runs/xepbr9fs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230602_193435-xepbr9fs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sweep_train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        # model = PhysicsGNN_LP(dataset, config.hidden_dim,\n",
    "        #                       config.num_layers, step=config.step)\n",
    "        # predictor = LinkPredictor(\n",
    "        #     config.hidden_dim, config.output_dim, config.mlp_layer, config.link_bias, config.dropout, device=device)\n",
    "        model = GNNStack(dataset.x.shape[1], config.hidden_dim, config.hidden_dim, config.num_layers, config.dropout, emb=True)\n",
    "   \n",
    "        predictor = LinkPredictor(\n",
    "            config.hidden_dim, config.hidden_dim, 1, config.num_layers+1,\n",
    "                    config.dropout)\n",
    "        pl_training_module = TrainingModule(\n",
    "            model, predictor, config.lr, config.wd)\n",
    "        exp_name = \"Sweep_LinkPred\"\n",
    "        wandb_logger = WandbLogger(\n",
    "            project=project_name, name=exp_name, config=hyperparameters)\n",
    "        trainer = trainer = pl.Trainer(\n",
    "            max_epochs=epochs,  # maximum number of epochs.\n",
    "            gpus=num_gpus,  # the number of gpus we have at our disposal.\n",
    "            default_root_dir=\"\", callbacks=[Get_Metrics(), EarlyStopping('Loss on test', mode='min', patience=15)],\n",
    "            logger=wandb_logger\n",
    "        )\n",
    "        trainer.fit(model=pl_training_module, datamodule=dataM)\n",
    "\n",
    "\n",
    "if mode == 'hp' and sweep:\n",
    "\n",
    "    import pprint\n",
    "\n",
    "    pprint.pprint(sweep_config)\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "\n",
    "    wandb.agent(sweep_id, sweep_train, count=500)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "if wb:\n",
    "    exp_name = \"Node_class_lr: \" + \\\n",
    "        str(hyperparameters['learning rate']) + \\\n",
    "        '_wd: ' + str(hyperparameters['weight decay'])\n",
    "    description = ' initial tests'\n",
    "    exp_name += description\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=project_name, name=exp_name, config=hyperparameters)\n",
    "\n",
    "\n",
    "trainer = trainer = pl.Trainer(\n",
    "    max_epochs=epochs,  # maximum number of epochs.\n",
    "    gpus=num_gpus,  # the number of gpus we have at our disposal.\n",
    "    default_root_dir=\"\", callbacks=[Get_Metrics(), EarlyStopping('Loss on test', mode='min', patience=15)],\n",
    "    logger=wandb_logger if wb else None\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | model     | GNNStack      | 996 K \n",
      "1 | predictor | LinkPredictor | 131 K \n",
      "--------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.515     Total estimated model params size (MB)\n",
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/peppe/Desktop/Università/Projects/Link_Prediction_with_Physics-based_GNNs-/src/link_prediction/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/anaconda3/envs/my_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s, loss=1.1, Loss on train=0.947, Loss on test=2.030, HR@100 on train=0.139, HR@100 on test=0.334]  \n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model = pl_training_module, datamodule = dataM)\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527\n"
     ]
    }
   ],
   "source": [
    "data = dataM.test_set\n",
    "out = pl_training_module.model(data)\n",
    "neg_shape = data.pos_edge_label_index.shape[1]\n",
    "print(neg_shape)\n",
    "preds = pl_training_module.predictor(out[data.pos_edge_label_index[0]], out[data.pos_edge_label_index[1]], training = False)\n",
    "preds_neg = pl_training_module.predictor(out[data.neg_edge_label_index[0][:neg_shape]], out[data.neg_edge_label_index[1][:neg_shape]], training = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2873, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.7865]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(preds_neg))\n",
    "pos = torch.tensor([[torch.mean(preds).item()]])\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([1.0000, 1.0000, 1.0000, 0.9999, 0.9996, 0.9996, 0.9993, 0.9992, 0.9991,\n",
      "        0.9991, 0.9990, 0.9989, 0.9988, 0.9985, 0.9977, 0.9969, 0.9969, 0.9967,\n",
      "        0.9958, 0.9956, 0.9944, 0.9935, 0.9904, 0.9892, 0.9884, 0.9875, 0.9867,\n",
      "        0.9857, 0.9855, 0.9826, 0.9825, 0.9815, 0.9780, 0.9779, 0.9772, 0.9759,\n",
      "        0.9739, 0.9711, 0.9710, 0.9702, 0.9699, 0.9653, 0.9644, 0.9642, 0.9608,\n",
      "        0.9532, 0.9526, 0.9515, 0.9508, 0.9498, 0.9491, 0.9486, 0.9425, 0.9412,\n",
      "        0.9387, 0.9367, 0.9351, 0.9343, 0.9302, 0.9298, 0.9218, 0.9215, 0.9187,\n",
      "        0.9106, 0.9057, 0.9031, 0.8982, 0.8964, 0.8930, 0.8911, 0.8821, 0.8799,\n",
      "        0.8798, 0.8776, 0.8722, 0.8697, 0.8686, 0.8681, 0.8679, 0.8497, 0.8402,\n",
      "        0.8359, 0.8336, 0.8321, 0.8287, 0.8279, 0.8212, 0.8202, 0.8116, 0.8090,\n",
      "        0.8065, 0.8041, 0.7950, 0.7949, 0.7942, 0.7865, 0.7862, 0.7783, 0.7764,\n",
      "        0.7757], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([432, 491, 210, 248, 391, 111, 356, 480, 437, 456, 235,  39, 169,  98,\n",
      "        457, 253, 411, 133, 152, 240, 261, 155, 310, 321, 217, 204, 314,  12,\n",
      "        103, 525, 445, 494, 272, 366, 429, 492, 401, 284, 126, 278, 487, 157,\n",
      "        147, 168, 170, 138, 224, 320, 202, 190,  85, 148, 151, 219, 428, 395,\n",
      "        160, 195, 393, 387, 291, 243, 484, 397, 182, 462,  58, 448, 430, 451,\n",
      "         42,   9, 186,  57,  26, 505, 326, 237,   2, 482, 254,  69, 294, 433,\n",
      "        328,  86, 474, 498, 175, 288,   1, 507,  94, 325, 267, 527,  78, 493,\n",
      "        105, 257]))\n",
      "inside\n"
     ]
    }
   ],
   "source": [
    "tot = torch.cat((preds_neg, pos), dim = 0)\n",
    "top = torch.topk(tot.squeeze(1), 100)\n",
    "print(top)\n",
    "\n",
    "if preds_neg.shape[0] in top.indices:\n",
    "    print(\"inside\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411765"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(preds, preds_neg, k = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
